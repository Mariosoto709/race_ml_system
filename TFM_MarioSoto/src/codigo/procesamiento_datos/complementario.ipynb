{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "athletes_files = glob.glob(os.path.join(data_dir, \"*.athletes.json\"))\n",
    "issues_files = glob.glob(os.path.join(data_dir, \"*.issues.json\"))\n",
    "\n",
    "all_athletes = {}\n",
    "all_issues = {}\n",
    "\n",
    "def get_race_id(filepath):\n",
    "    filename = os.path.basename(filepath)\n",
    "    return filename.split(\".\")[0]\n",
    "\n",
    "for a_file in athletes_files:\n",
    "    race_id = get_race_id(a_file)\n",
    "    with open(a_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        all_athletes[race_id] = json.load(f)\n",
    "\n",
    "for i_file in issues_files:\n",
    "    race_id = get_race_id(i_file)\n",
    "    with open(i_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        all_issues[race_id] = json.load(f)\n",
    "\n",
    "print(\"Carreras cargadas:\", list(all_athletes.keys()))\n",
    "print(\"Ejemplo atletas primera carrera:\", all_athletes[list(all_athletes.keys())[0]][:3])\n",
    "print(\"Ejemplo issues primera carrera:\", all_issues[list(all_issues.keys())[0]][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def merge_structures(base, new, max_unique=0):\n",
    "\n",
    "    if isinstance(base, dict) and isinstance(new, dict):\n",
    "        # si ambos son diccionarios, combinar claves recursivamente\n",
    "        merged = dict(base)\n",
    "        for key, val in new.items():\n",
    "            if key in merged:\n",
    "                merged[key] = merge_structures(merged[key], val, max_unique)\n",
    "            else:\n",
    "                merged[key] = val\n",
    "        return merged\n",
    "\n",
    "    elif isinstance(base, set) and isinstance(new, set):\n",
    "        # si ambos son conjuntos, unir y limitar tamaño si es necesario\n",
    "        merged_set = base | new\n",
    "        if len(merged_set) > max_unique:\n",
    "            return \"*\"\n",
    "        return merged_set\n",
    "\n",
    "    elif isinstance(base, set):\n",
    "        # caso mixto: base es un set y new no\n",
    "        if isinstance(new, (dict, list)):\n",
    "            return \"*|dict_or_list\"\n",
    "        return merge_structures(base, {new}, max_unique)\n",
    "\n",
    "    elif isinstance(new, set):\n",
    "        # caso mixto inverso: new es set y base no\n",
    "        if isinstance(base, (dict, list)):\n",
    "            return \"*|dict_or_list\"\n",
    "        return merge_structures({base}, new, max_unique)\n",
    "\n",
    "    else:\n",
    "        # caso base: ambos son valores escalares (int, str, etc.)\n",
    "        if isinstance(base, (dict, list)) or isinstance(new, (dict, list)):\n",
    "            return \"*|dict_or_list\"\n",
    "        return {base, new}\n",
    "\n",
    "\n",
    "def extract_structure_unique(data):\n",
    "    \n",
    "    if isinstance(data, dict):\n",
    "        return {k: extract_structure_unique(v) for k, v in data.items()}\n",
    "\n",
    "    elif isinstance(data, list):\n",
    "        if not data:\n",
    "            return {\"[]\": {}}  # lista vacía, estructura vacía\n",
    "        # empezamos con la estructura del primer elemento\n",
    "        struct = extract_structure_unique(data[0])\n",
    "        # combinamos estructuras del resto de elementos\n",
    "        for item in data[1:]:\n",
    "            struct = merge_structures(struct, extract_structure_unique(item))\n",
    "        return {\"[]\": struct}\n",
    "\n",
    "    else:\n",
    "        # valor escalar (str, int, bool, None, etc.)\n",
    "        return {data}\n",
    "\n",
    "\n",
    "def sets_to_lists(obj):\n",
    "    \n",
    "    if isinstance(obj, dict):\n",
    "        return {k: sets_to_lists(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, set):\n",
    "        return [sets_to_lists(v) for v in obj]\n",
    "    elif isinstance(obj, list):\n",
    "        return [sets_to_lists(v) for v in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "\n",
    "athletes_files = glob.glob(os.path.join(data_dir, \"*.athletes.json\"))\n",
    "\n",
    "output_dir = os.path.join(data_dir, \"estructuras\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def get_race_id(filepath):\n",
    "    \"\"\"Extrae el ID de carrera a partir del nombre del archivo.\"\"\"\n",
    "    return os.path.basename(filepath).split(\".\")[0]\n",
    "\n",
    "\n",
    "for a_file in athletes_files:\n",
    "    race_id = get_race_id(a_file)\n",
    "    print(f\"\\nProcesando carrera: {race_id}\")\n",
    "\n",
    "    # procesamiento de ATHLETES\n",
    "    with open(a_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        athletes = json.load(f)\n",
    "\n",
    "    structure = {}\n",
    "    # fusionar estructuras de todos los registros\n",
    "    for record in athletes:\n",
    "        structure = merge_structures(structure, extract_structure_unique(record))\n",
    "\n",
    "    structure_serializable = sets_to_lists(structure)\n",
    "\n",
    "    # guardar estructura en archivo JSON\n",
    "    output_path_ath = os.path.join(output_dir, f\"{race_id}_structure_athletes.json\")\n",
    "    with open(output_path_ath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(structure_serializable, f, indent=4, ensure_ascii=False)\n",
    "    print(f\"Estructura ATHLETES guardada en: {output_path_ath}\")\n",
    "\n",
    "    # procesamiento de issues\n",
    "    i_file = a_file.replace(\".athletes.json\", \".issues.json\")\n",
    "\n",
    "    if os.path.exists(i_file):\n",
    "        with open(i_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            issues = json.load(f)\n",
    "\n",
    "        structure_issues = {}\n",
    "        for record in issues:\n",
    "            structure_issues = merge_structures(structure_issues, extract_structure_unique(record))\n",
    "\n",
    "        structure_issues_serializable = sets_to_lists(structure_issues)\n",
    "\n",
    "        output_path_iss = os.path.join(output_dir, f\"{race_id}_structure_issues.json\")\n",
    "        with open(output_path_iss, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(structure_issues_serializable, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(f\"Estructura ISSUES guardada en: {output_path_iss}\")\n",
    "    else:\n",
    "        print(\"No se encontró archivo ISSUES para esta carrera.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CAMBIARLO PARA QUE SE UED HACER POR CARRERA TAMBIEN\n",
    "\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "\n",
    "ATHLETE_ID = 'L6554352'  # cambia por el ID del atleta que quieras\n",
    "\n",
    "# obtener archivos\n",
    "athletes_files = glob.glob(os.path.join(data_dir, \"*.athletes.json\"))\n",
    "issues_files = glob.glob(os.path.join(data_dir, \"*.issues.json\"))\n",
    "\n",
    "if not athletes_files:\n",
    "    raise FileNotFoundError(\"No se encontró ningún archivo de atletas en el directorio.\")\n",
    "if not issues_files:\n",
    "    raise FileNotFoundError(\"No se encontró ningún archivo de issues en el directorio.\")\n",
    "\n",
    "# carga de datos\n",
    "for fpath in athletes_files:\n",
    "    with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "        athletes_data.extend(json.load(f))  # agregamos todos los atletas de cada archivo\n",
    "\n",
    "issues_data = []\n",
    "for fpath in issues_files:\n",
    "    with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "        issues_data.extend(json.load(f))  # agregamos todas las listas de issues\n",
    "\n",
    "#buscarlo\n",
    "athlete = next((a for a in athletes_data if str(a.get(\"id\")) == str(ATHLETE_ID)), None)\n",
    "if athlete is None:\n",
    "    raise ValueError(f\"No se encontró ningún atleta con id {ATHLETE_ID}\")\n",
    "\n",
    "#busar issues\n",
    "for issue_list in issues_data:\n",
    "    # cada issue_list puede ser una lista de issues\n",
    "    if isinstance(issue_list, list):\n",
    "        for issue in issue_list:\n",
    "            if str(issue.get(\"athlete_id\")) == str(ATHLETE_ID) or issue.get(\"athlete_id\") == athlete.get(\"id\"):\n",
    "                athlete_issues.append(issue)\n",
    "    elif isinstance(issue_list, dict):\n",
    "        # si algún archivo tiene un dict en lugar de lista\n",
    "        if str(issue_list.get(\"athlete_id\")) == str(ATHLETE_ID):\n",
    "            athlete_issues.append(issue_list)\n",
    "\n",
    "output_athlete = os.path.join(data_dir, f\"athlete_{ATHLETE_ID}.json\")\n",
    "output_issues = os.path.join(data_dir, f\"athlete_{ATHLETE_ID}_issues.json\")\n",
    "\n",
    "with open(output_athlete, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(athlete, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(output_issues, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(athlete_issues, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Archivos guardados:\\n - {output_athlete}\\n - {output_issues}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes_time_df = dfs['athletes_time_df']\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "numeric_pairs = [\n",
    "    (\"netTime\", \"time\"),\n",
    "    (\"originalTime\", \"raw_times_official\"),\n",
    "    (\"originalTime\", \"raw_times_real\"),\n",
    "    (\"raw_backupOffset\", \"offset\"),\n",
    "    (\"raw_times_official\", \"raw_times_real\"),\n",
    "    (\"raw_times_rawTime\", \"raw_rawTime\"),\n",
    "]\n",
    "\n",
    "for col1, col2 in numeric_pairs:\n",
    "    df_pair = athletes_time_df[[col1, col2]].dropna()\n",
    "    exact_match = (df_pair[col1] == df_pair[col2]).all()\n",
    "    diff_mean = (df_pair[col1] - df_pair[col2]).abs().mean()\n",
    "    print(f\"{col1} vs {col2}: Exact match? {exact_match}, Mean absolute difference: {diff_mean}\")\n",
    "\n",
    "datetime_pairs = [\n",
    "    (\"rawTime\", \"raw_rawTime\"),\n",
    "    (\"raw_originalTime\", \"raw_times_rawTime\"),\n",
    "]\n",
    "\n",
    "for col1, col2 in datetime_pairs:\n",
    "    df_pair = athletes_time_df[[col1, col2]].dropna()\n",
    "    exact_match = (df_pair[col1] == df_pair[col2]).all()\n",
    "    diff_mean = (df_pair[col1] - df_pair[col2]).abs().mean()\n",
    "    print(f\"{col1} vs {col2}: Exact match? {exact_match}, Mean absolute difference: {diff_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_folder = r\"C:\\Users\\mario\\Desktop\\MasterCienciadeDatos\\TFM\\TFM_MarioSoto\\raiz\\datos\\historicos\\calcular_peso\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for name, df in dfs_final.items():\n",
    "    print(f\"Guardando {name}...\")\n",
    "\n",
    "    parquet_path = os.path.join(output_folder, f\"{name}.parquet\")\n",
    "    df.to_parquet(parquet_path, index=False)\n",
    "    \n",
    "    csv_path = os.path.join(output_folder, f\"{name}.csv.gz\")\n",
    "    df.to_csv(csv_path, index=False, compression='gzip')\n",
    "\n",
    "print(\"\\n✅ Todos los DataFrames se han guardado correctamente en:\")\n",
    "print(output_folder)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
